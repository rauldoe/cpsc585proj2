{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# Importing the EMNIST letters\n",
    "from scipy import io as sio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size of 128 had about 90.4% accuracy.\n",
    "# Thus, a batch size of 1000 was used where accuracy was about 91.5%. \n",
    "# Signifigantly higher batch sizes also decreased test accuracy.\n",
    "# A batch size of 104,000 led to an accuracy of about \n",
    "batch_size = 1000\n",
    "# num_classes = 10\n",
    "num_classes = 26\n",
    "epochs = 1000 #There is early stopping, so it won't reach 1000 epochs. This needs to be high.\n",
    "\n",
    "# https://stackoverflow.com/questions/51125969/loading-emnist-letters-dataset/53547262#53547262\n",
    "mat = sio.loadmat('emnist-letters.mat')\n",
    "data = mat['dataset']\n",
    "\n",
    "x_train = data['train'][0,0]['images'][0,0]\n",
    "y_train = data['train'][0,0]['labels'][0,0]\n",
    "x_test = data['test'][0,0]['images'][0,0]\n",
    "y_test = data['test'][0,0]['labels'][0,0]\n",
    "\n",
    "val_start = x_train.shape[0] - x_test.shape[0]\n",
    "x_val = x_train[val_start:x_train.shape[0],:]\n",
    "y_val = y_train[val_start:x_train.shape[0]]\n",
    "x_train = x_train[0:val_start,:]\n",
    "y_train = y_train[0:val_start]\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = tf.keras.utils.to_categorical(y_train - 1, num_classes, dtype='float32')\n",
    "y_test = tf.keras.utils.to_categorical(y_test - 1, num_classes, dtype='float32')\n",
    "\n",
    "y_val = tf.keras.utils.to_categorical(y_val - 1, num_classes, dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2000)              1026000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 26)                52026     \n",
      "=================================================================\n",
      "Total params: 1,479,946\n",
      "Trainable params: 1,479,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Sigmoid seemed to work better for test accuracy compared to relu. (sigmoid was getting 91% test accuracy compared to 89% for relu.)\n",
    "# Sigmoid was slighly better than tanh, but both were about the same test accuracy (within a few tenths of a percent)\n",
    "model.add(Dense(512, activation='sigmoid', input_shape=(784,)))\n",
    "# Tried different dropout rates, but 0.2 seemed to work well and provided a modest improvement.\n",
    "# (~0.5% test accuracy improvement compared to not using dropout at all)\n",
    "model.add(Dropout(0.2))\n",
    "# Compared to other numbers of neurons, this number seemed to work well (2000 hidden neurons)\n",
    "model.add(Dense(2000, activation='sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n",
    "earlyStop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0.0001, patience=5, verbose=0, mode='auto',\n",
    "    baseline=None, restore_best_weights=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 104000 samples, validate on 20800 samples\n",
      "Epoch 1/1000\n",
      "104000/104000 [==============================] - 18s 175us/sample - loss: 1.7122 - accuracy: 0.5154 - val_loss: 1.0846 - val_accuracy: 0.6675\n",
      "Epoch 2/1000\n",
      "104000/104000 [==============================] - 18s 169us/sample - loss: 0.9819 - accuracy: 0.7004 - val_loss: 0.8357 - val_accuracy: 0.7478\n",
      "Epoch 3/1000\n",
      "104000/104000 [==============================] - 19s 179us/sample - loss: 0.7956 - accuracy: 0.7552 - val_loss: 0.6756 - val_accuracy: 0.7929\n",
      "Epoch 4/1000\n",
      "104000/104000 [==============================] - 18s 169us/sample - loss: 0.6910 - accuracy: 0.7855 - val_loss: 0.6021 - val_accuracy: 0.8138\n",
      "Epoch 5/1000\n",
      "104000/104000 [==============================] - 17s 164us/sample - loss: 0.6220 - accuracy: 0.8048 - val_loss: 0.5462 - val_accuracy: 0.8294\n",
      "Epoch 6/1000\n",
      "104000/104000 [==============================] - 18s 175us/sample - loss: 0.5751 - accuracy: 0.8188 - val_loss: 0.5193 - val_accuracy: 0.8400\n",
      "Epoch 7/1000\n",
      "104000/104000 [==============================] - 17s 163us/sample - loss: 0.5367 - accuracy: 0.8303 - val_loss: 0.4686 - val_accuracy: 0.8537\n",
      "Epoch 8/1000\n",
      "104000/104000 [==============================] - 18s 171us/sample - loss: 0.5058 - accuracy: 0.8393 - val_loss: 0.4521 - val_accuracy: 0.8573\n",
      "Epoch 9/1000\n",
      "104000/104000 [==============================] - 18s 172us/sample - loss: 0.4826 - accuracy: 0.8451 - val_loss: 0.4297 - val_accuracy: 0.8635\n",
      "Epoch 10/1000\n",
      "104000/104000 [==============================] - 19s 179us/sample - loss: 0.4573 - accuracy: 0.8525 - val_loss: 0.4111 - val_accuracy: 0.8710\n",
      "Epoch 11/1000\n",
      "104000/104000 [==============================] - 17s 168us/sample - loss: 0.4390 - accuracy: 0.8586 - val_loss: 0.3922 - val_accuracy: 0.8727\n",
      "Epoch 12/1000\n",
      "104000/104000 [==============================] - 17s 165us/sample - loss: 0.4226 - accuracy: 0.8627 - val_loss: 0.3855 - val_accuracy: 0.8770\n",
      "Epoch 13/1000\n",
      "104000/104000 [==============================] - 17s 167us/sample - loss: 0.4046 - accuracy: 0.8692 - val_loss: 0.3785 - val_accuracy: 0.8775\n",
      "Epoch 14/1000\n",
      "104000/104000 [==============================] - 17s 167us/sample - loss: 0.3938 - accuracy: 0.8720 - val_loss: 0.3558 - val_accuracy: 0.8886\n",
      "Epoch 15/1000\n",
      "104000/104000 [==============================] - 17s 165us/sample - loss: 0.3838 - accuracy: 0.8750 - val_loss: 0.3567 - val_accuracy: 0.8840\n",
      "Epoch 16/1000\n",
      "104000/104000 [==============================] - 17s 166us/sample - loss: 0.3677 - accuracy: 0.8789 - val_loss: 0.3391 - val_accuracy: 0.8927\n",
      "Epoch 17/1000\n",
      "104000/104000 [==============================] - 17s 167us/sample - loss: 0.3605 - accuracy: 0.8804 - val_loss: 0.3357 - val_accuracy: 0.8904\n",
      "Epoch 18/1000\n",
      "104000/104000 [==============================] - 18s 174us/sample - loss: 0.3462 - accuracy: 0.8857 - val_loss: 0.3287 - val_accuracy: 0.8932\n",
      "Epoch 19/1000\n",
      "104000/104000 [==============================] - 20s 191us/sample - loss: 0.3371 - accuracy: 0.8883 - val_loss: 0.3185 - val_accuracy: 0.8970\n",
      "Epoch 20/1000\n",
      "104000/104000 [==============================] - 22s 213us/sample - loss: 0.3310 - accuracy: 0.8912 - val_loss: 0.3217 - val_accuracy: 0.8966\n",
      "Epoch 21/1000\n",
      "104000/104000 [==============================] - 25s 239us/sample - loss: 0.3225 - accuracy: 0.8933 - val_loss: 0.3141 - val_accuracy: 0.8978\n",
      "Epoch 22/1000\n",
      "104000/104000 [==============================] - 22s 216us/sample - loss: 0.3157 - accuracy: 0.8949 - val_loss: 0.3126 - val_accuracy: 0.8988\n",
      "Epoch 23/1000\n",
      "104000/104000 [==============================] - 19s 186us/sample - loss: 0.3114 - accuracy: 0.8961 - val_loss: 0.3070 - val_accuracy: 0.9006\n",
      "Epoch 24/1000\n",
      "104000/104000 [==============================] - 21s 203us/sample - loss: 0.3034 - accuracy: 0.8989 - val_loss: 0.3030 - val_accuracy: 0.9016\n",
      "Epoch 25/1000\n",
      "104000/104000 [==============================] - 21s 199us/sample - loss: 0.3008 - accuracy: 0.8996 - val_loss: 0.2947 - val_accuracy: 0.9047\n",
      "Epoch 26/1000\n",
      "104000/104000 [==============================] - 20s 188us/sample - loss: 0.2942 - accuracy: 0.9018 - val_loss: 0.3011 - val_accuracy: 0.9032\n",
      "Epoch 27/1000\n",
      "104000/104000 [==============================] - 19s 179us/sample - loss: 0.2885 - accuracy: 0.9031 - val_loss: 0.2902 - val_accuracy: 0.9056\n",
      "Epoch 28/1000\n",
      "104000/104000 [==============================] - 19s 185us/sample - loss: 0.2826 - accuracy: 0.9048 - val_loss: 0.2897 - val_accuracy: 0.9069\n",
      "Epoch 29/1000\n",
      "104000/104000 [==============================] - 19s 179us/sample - loss: 0.2811 - accuracy: 0.9053 - val_loss: 0.2896 - val_accuracy: 0.9050\n",
      "Epoch 30/1000\n",
      "104000/104000 [==============================] - 18s 172us/sample - loss: 0.2751 - accuracy: 0.9067 - val_loss: 0.2799 - val_accuracy: 0.9094\n",
      "Epoch 31/1000\n",
      "104000/104000 [==============================] - 19s 180us/sample - loss: 0.2680 - accuracy: 0.9084 - val_loss: 0.2858 - val_accuracy: 0.9063\n",
      "Epoch 32/1000\n",
      "104000/104000 [==============================] - 19s 179us/sample - loss: 0.2665 - accuracy: 0.9097 - val_loss: 0.2818 - val_accuracy: 0.9098\n",
      "Epoch 33/1000\n",
      "104000/104000 [==============================] - 18s 176us/sample - loss: 0.2633 - accuracy: 0.9094 - val_loss: 0.2835 - val_accuracy: 0.9077\n",
      "Epoch 34/1000\n",
      "104000/104000 [==============================] - 19s 182us/sample - loss: 0.2601 - accuracy: 0.9113 - val_loss: 0.2776 - val_accuracy: 0.9104\n",
      "Epoch 35/1000\n",
      "104000/104000 [==============================] - 17s 163us/sample - loss: 0.2558 - accuracy: 0.9122 - val_loss: 0.2754 - val_accuracy: 0.9108\n",
      "Epoch 36/1000\n",
      "104000/104000 [==============================] - 17s 166us/sample - loss: 0.2508 - accuracy: 0.9140 - val_loss: 0.2735 - val_accuracy: 0.9126\n",
      "Epoch 37/1000\n",
      "104000/104000 [==============================] - 18s 173us/sample - loss: 0.2489 - accuracy: 0.9153 - val_loss: 0.2733 - val_accuracy: 0.9104\n",
      "Epoch 38/1000\n",
      "104000/104000 [==============================] - 21s 198us/sample - loss: 0.2455 - accuracy: 0.9149 - val_loss: 0.2747 - val_accuracy: 0.9108\n",
      "Epoch 39/1000\n",
      "104000/104000 [==============================] - 19s 178us/sample - loss: 0.2432 - accuracy: 0.9155 - val_loss: 0.2773 - val_accuracy: 0.9117\n",
      "Epoch 40/1000\n",
      "104000/104000 [==============================] - 18s 171us/sample - loss: 0.2378 - accuracy: 0.9180 - val_loss: 0.2702 - val_accuracy: 0.9113\n",
      "Epoch 41/1000\n",
      "104000/104000 [==============================] - 21s 198us/sample - loss: 0.2358 - accuracy: 0.9187 - val_loss: 0.2649 - val_accuracy: 0.9138\n",
      "Epoch 42/1000\n",
      "104000/104000 [==============================] - 19s 186us/sample - loss: 0.2322 - accuracy: 0.9196 - val_loss: 0.2704 - val_accuracy: 0.9126\n",
      "Epoch 43/1000\n",
      "104000/104000 [==============================] - 18s 178us/sample - loss: 0.2303 - accuracy: 0.9193 - val_loss: 0.2722 - val_accuracy: 0.9123\n",
      "Epoch 44/1000\n",
      "104000/104000 [==============================] - 18s 169us/sample - loss: 0.2274 - accuracy: 0.9216 - val_loss: 0.2675 - val_accuracy: 0.9149\n",
      "Epoch 45/1000\n",
      "104000/104000 [==============================] - 17s 162us/sample - loss: 0.2212 - accuracy: 0.9215 - val_loss: 0.2627 - val_accuracy: 0.9140\n",
      "Epoch 46/1000\n",
      "104000/104000 [==============================] - 16s 154us/sample - loss: 0.2211 - accuracy: 0.9222 - val_loss: 0.2633 - val_accuracy: 0.9170\n",
      "Epoch 47/1000\n",
      "104000/104000 [==============================] - 18s 168us/sample - loss: 0.2185 - accuracy: 0.9230 - val_loss: 0.2634 - val_accuracy: 0.9164\n",
      "Epoch 48/1000\n",
      "104000/104000 [==============================] - 18s 177us/sample - loss: 0.2198 - accuracy: 0.9234 - val_loss: 0.2598 - val_accuracy: 0.9163\n",
      "Epoch 49/1000\n",
      "104000/104000 [==============================] - 18s 172us/sample - loss: 0.2169 - accuracy: 0.9243 - val_loss: 0.2633 - val_accuracy: 0.9164\n",
      "Epoch 50/1000\n",
      "104000/104000 [==============================] - 18s 175us/sample - loss: 0.2142 - accuracy: 0.9250 - val_loss: 0.2613 - val_accuracy: 0.9178\n",
      "Epoch 51/1000\n",
      "104000/104000 [==============================] - 18s 174us/sample - loss: 0.2092 - accuracy: 0.9258 - val_loss: 0.2619 - val_accuracy: 0.9156\n",
      "Epoch 52/1000\n",
      "104000/104000 [==============================] - 18s 171us/sample - loss: 0.2098 - accuracy: 0.9256 - val_loss: 0.2648 - val_accuracy: 0.9166\n",
      "Epoch 53/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104000/104000 [==============================] - 18s 174us/sample - loss: 0.2061 - accuracy: 0.9267 - val_loss: 0.2620 - val_accuracy: 0.9172\n",
      "Test loss: 0.26889752828014585\n",
      "Test accuracy: 0.91533655\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs, callbacks=[earlyStop],\n",
    "                    validation_data=(x_val, y_val)\n",
    "                    )\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
